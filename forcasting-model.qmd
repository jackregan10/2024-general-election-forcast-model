---
title: "2024 General Election Forcasting Model"
subtitle: "POLSCI 239 - Assignment Four"
author: "Jack Regan"
format: pdf
editor: visual
---


```{r}
#| label: load-libraries
#| message: false
#| echo: false

library(tidyverse)
library(ggplot2)
library(readr)
library(janitor)
library(stats)
```

# Methodology

The data for this model is borrowed from ABC's 538 general election state polling dataset. (Full citation in README)
```{r}
#| label: read-polling-data
polling_data <- read_csv("data/president_polls.csv", show_col_types = FALSE)
glimpse(polling_data)
```
\newpage

# Data Cleaning

The model will only calculate win percentages for toss up states.

```{r}
#| lable: select-columns-and-filter
toss_up_states <- c("Michigan", "Nevada", 
      "Arizona", "New Mexico", 
      "Wisconsin", "Pennsylvania", 
      "North Carolina", "Georgia")

polling_data <- polling_data |>
  select(
    poll_id,
    state,
    end_date,
    sample_size,
    candidate_name,
    pct
  ) |>
  rename("dem_pct" = pct) |>
  filter(candidate_name == "Kamala Harris" & state %in% toss_up_states) |>
  mutate(end_date = as.Date(end_date, format = "%m/%d/%y")) |>
  arrange(end_date) |>
  drop_na(sample_size)

glimpse(polling_data)
```
\newpage

# Summary Statistics

```{r}
#| label: summary-statistics

options(pillar.sigfig = 7)

polling_data |>
  group_by(state) |>
  summarise(
    poll_count = n(),
    raw_harris_approval = mean(dem_pct),
    ealiest_poll = min(end_date),
    most_recent_poll = max(end_date)
  )
```

\newpage 

# Weighting Data by Sample Size

Each poll was weighted using a function based on its sample size. Specifically, I take the square root of the median sample size for each state and then multipled the Harris approval percentage for each poll by the square root of the poll's sample size divided by that states' square-rooted median sample size. This methodology was adopted from 538's weighting guidelines and then adjusted to fit the specifications of the dataset. A new "adjusted_pct" variable was applied to each poll in the dataset.

```{r}
#| lable: cleaning-data

options(pillar.sigfig = 7)

square_root_median_sample_size_by_state <- polling_data |>
  group_by(state) |>
  summarize(
    square_root_median_sample_size = sqrt(median(sample_size, na.rm = TRUE))
  )
as_tibble(square_root_median_sample_size_by_state)

polling_data <- polling_data |>
  mutate(adjusted_pct = case_when(
    state == "Arizona" ~ sqrt(sample_size)/27.85678*dem_pct,
    state == "Georgia" ~ sqrt(sample_size)/28.26659*dem_pct,
    state == "Michigan" ~ sqrt(sample_size)/26.22975*dem_pct,
    state == "Nevada" ~ sqrt(sample_size)/26.01922*dem_pct,
    state == "New Mexico" ~ sqrt(sample_size)/22.94559*dem_pct,
    state == "North Carolina" ~ sqrt(sample_size)/28.28427*dem_pct,
    state == "Pennsylvania" ~ sqrt(sample_size)/28.33725*dem_pct,
    state == "Wisconsin" ~ sqrt(sample_size)/26.45751*dem_pct
    )
  )

glimpse(polling_data)
```

\newpage

# Exponentially Weighted Moving Average

In an EWMA calculation, recent data points are assigned more weight than older points. This makes the average more responsive to recent changes in the data. The lambda variable controls how much weight is assigned to more recent data points. I assign a lambda value of 0.95 in order to assign greater importance to more recent polls. The smoothed average provides the a value for Harris' approval rating that is then used as the Dem win percentage in my forcast. More documentation on the EWMA can be found here (https://www.investopedia.com/articles/07/ewma.asp).

```{r}
#| label: calculate-margin-by-state
#| message: false

options(pillar.sigfig = 7)

calculate_ewma <- function(data, raw_average, lambda) {
  
  ewma <- numeric(length(data[[raw_average]]))
  ewma[1] <- data[[raw_average]][1]
  
  for (i in 2:length(data[[raw_average]])) {
    ewma[i] <- lambda * data[[raw_average]][i] + (1 - lambda) * ewma[i - 1]
  }
  return(ewma[length(ewma)])
}
polling_data |>
  group_by(state) |>
  summarise(
    ewma_adjusted_pct = calculate_ewma(cur_data(), "adjusted_pct", 0.95)
  )
```

\newpage

# Additional Considerations and Data Limitations

This dataset introduces several inconsistencies to the model which will be addressed here. First, the inconsistent number of polls conducted within each state creates uncertainty in the accuracy of the data. Second, the variability of polling sources opens the data to potential bias. FiveThirtyEight uses extensive guidelines when choosing polls to include within their data in order to account for bias; however, this is mostly a subjective science and isn't statistically grounded in my model. Information on 538's polling policy can be found here (https://fivethirtyeight.com/features/polls-policy-and-faqs/). Third, this model uses a ruidmentary modeling algorithm that adjusts based on sample size and time decay. Other weights such as pollster rating and margin of error are common strategies, but are not considered in this model.

Weighting and averaging data admits a certain level of subjectivity into the data as the methods by which the data is adjusted are largely statistically insignificant. The weighting and averaging methods I chose were subjective choices influenced by common practice but are not scientifically grounded as the best practice.
